{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4f6a0ac8-ec91-4314-891f-f50a6e301f51",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\tneel\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\tneel\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\tneel\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#importing necessary libraries\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "\n",
    "# Download necessary NLTK data\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "43f55f6a-bb1d-4692-8028-82710cf9c87c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize lemmatizer and stopwords\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Define preprocessing function\n",
    "def preprocess_text(text):\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove special characters and numbers\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)\n",
    "    # Tokenize, remove stopwords, and lemmatize\n",
    "    text = ' '.join([lemmatizer.lemmatize(word) for word in text.split() if word not in stop_words])\n",
    "    return text\n",
    "\n",
    "df = pd.read_csv(r'C:\\Users\\tneel\\Downloads\\final_project_sentiment_analysis\\archive\\Reviews.csv')\n",
    "\n",
    "# Apply preprocessing to the 'Text' column\n",
    "df['cleaned_text'] = df['Text'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4b705d93-5467-4fbc-b1f4-92163aa46272",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binary sentiment classification\n",
    "def assign_sentiment(score):\n",
    "    if score > 3:\n",
    "        return 1  # Positive\n",
    "    elif score < 3:\n",
    "        return 0  # Negative\n",
    "    else:\n",
    "        return None  # Neutral (can drop these if focusing on binary classification)\n",
    "\n",
    "df['sentiment'] = df['Score'].apply(assign_sentiment)\n",
    "\n",
    "# Drop rows with neutral sentiment\n",
    "df = df.dropna(subset=['sentiment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d656bf0b-2d56-4b38-af73-3a3a3097b74a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the data into train and test sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x = df['cleaned_text']\n",
    "y = df['sentiment']\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7a69a36e-17a9-466e-8f60-0ef2bdf7509b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Vectorize the text data using TF-IDF\n",
    "vectorizer = TfidfVectorizer(max_features=5000)  # Limit features to 5000\n",
    "x_train_tfidf = vectorizer.fit_transform(x_train)\n",
    "x_test_tfidf = vectorizer.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dbd6eec6-3430-4772-830b-77750d35bcbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.84      0.68      0.75     16407\n",
      "         1.0       0.94      0.98      0.96     88756\n",
      "\n",
      "    accuracy                           0.93    105163\n",
      "   macro avg       0.89      0.83      0.86    105163\n",
      "weighted avg       0.93      0.93      0.93    105163\n",
      "\n",
      "Confusion Matrix:\n",
      " [[11232  5175]\n",
      " [ 2209 86547]]\n",
      "Accuracy Score: 0.9297851906088643\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "# Train logistic regression\n",
    "log_reg = LogisticRegression(max_iter=1000)\n",
    "log_reg.fit(x_train_tfidf, y_train)\n",
    "\n",
    "# Predictions and evaluation\n",
    "y_pred = log_reg.predict(x_test_tfidf)\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "print(\"Accuracy Score:\", accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ab3c9d1a-0d36-41d0-8560-2df59c884608",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 Performing Products (Based on Sentiment):\n",
      "         ProductId  sentiment  \\\n",
      "156     B0036VM05I        1.0   \n",
      "35633   B0036UQTQA        1.0   \n",
      "36332   B0036UWPJ0        1.0   \n",
      "105943  B0036VE3V2        1.0   \n",
      "228469  B0036V7K4E        1.0   \n",
      "229039  B0036VFX5M        1.0   \n",
      "309833  B0036UV8RU        1.0   \n",
      "487525  B001XWR35Y        1.0   \n",
      "539149  B0036VFTMO        1.0   \n",
      "544019  B0036UUY9I        1.0   \n",
      "\n",
      "                                                  Summary  \n",
      "156                                            Great Deal  \n",
      "35633                         New label, same great stuff  \n",
      "36332                                   Works really well  \n",
      "105943                   My Dog Thinks They Are The Best!  \n",
      "228469  You can't buy cheddar like this at the grocery...  \n",
      "229039                    Not for everyone but I LOVE it!  \n",
      "309833                            I love the coffee, but:  \n",
      "487525                          Best Risotto in the World  \n",
      "539149                                Dog + cats LOVE it!  \n",
      "544019                                           YUMMMMMY  \n",
      "Worst 10 Performing Products (Based on Sentiment):\n",
      "         ProductId  sentiment  \\\n",
      "93743   B00514BIQK        0.0   \n",
      "173990  B005116LQU        0.0   \n",
      "210927  B0025VF8TK        0.0   \n",
      "226725  B0025UOODC        0.0   \n",
      "312697  B004TMAAM8        0.0   \n",
      "369755  B0015DEH8C        0.0   \n",
      "369936  B0015DEH8W        0.0   \n",
      "396416  B0025VP9I0        0.0   \n",
      "456925  B005116KK2        0.0   \n",
      "525741  B005116LMO        0.0   \n",
      "\n",
      "                                                  Summary  \n",
      "93743                      WARNING to SENSITIVE Celiacs!!  \n",
      "173990                             Lowell Foods Dew Sugar  \n",
      "210927  Doesn't look like a salmon, doesn't taste like...  \n",
      "226725                               Terrible After Taste  \n",
      "312697                                      Harm no Charm  \n",
      "369755                       Do not order... stale candy!  \n",
      "369936     Sent an open package containing only 28 pieces  \n",
      "396416                  item not as shown or name ordered  \n",
      "456925                               Worst pork pate ever  \n",
      "525741                          Disappointed with quality  \n"
     ]
    }
   ],
   "source": [
    "# identifying top and worst performing products\n",
    "product_sentiment = df.groupby('ProductId')['sentiment'].mean().reset_index()\n",
    "product_sentiment_sorted = product_sentiment.sort_values(by='sentiment', ascending=False)\n",
    "\n",
    "top_performing_products = product_sentiment_sorted.head(10)\n",
    "worst_performing_products = product_sentiment_sorted.tail(10)\n",
    "\n",
    "top_products_info = df[df['ProductId'].isin(top_performing_products['ProductId'])]\n",
    "worst_products_info = df[df['ProductId'].isin(worst_performing_products['ProductId'])]\n",
    "\n",
    "print(\"Top 10 Performing Products (Based on Sentiment):\")\n",
    "top_info = top_products_info[['ProductId', 'sentiment', 'Summary']].drop_duplicates(subset='ProductId')\n",
    "print(top_info)\n",
    "\n",
    "print(\"Worst 10 Performing Products (Based on Sentiment):\")\n",
    "worst_info = worst_products_info[['ProductId', 'sentiment', 'Summary']].drop_duplicates(subset='ProductId')\n",
    "print(worst_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c522ff4-bd9b-47c2-b49f-9c209a117ab4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
